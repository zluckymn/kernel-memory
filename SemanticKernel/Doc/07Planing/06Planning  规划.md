# Planning  è§„åˆ’

- 06/11/2025

 

Once you have multiple plugins, you then need a way for your AI agent to use them together to solve a userâ€™s need. This is where planning comes in.
ä¸€æ—¦ä½ æœ‰äº†å¤šä¸ªæ’ä»¶ï¼Œä½ å°±éœ€è¦ä¸€ç§æ–¹æ³•è®©ä½ çš„ AI ä»£ç†å°†å®ƒä»¬ä¸€èµ·ä½¿ç”¨æ¥è§£å†³ç”¨æˆ·çš„éœ€æ±‚ã€‚è¿™å°±æ˜¯è§„åˆ’çš„ç”¨æ­¦ä¹‹åœ°ã€‚

Early on, Semantic Kernel introduced the concept of planners that used prompts to request the AI to choose which functions to invoke. Since Semantic Kernel was introduced, however, OpenAI introduced a native way for the model to invoke or â€œcallâ€ a function: [function calling](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/function-calling/). Other AI models like Gemini, Claude, and Mistral have since adopted function calling as a core capability, making it a cross-model supported feature.
æ—©æœŸï¼ŒSemantic Kernelå¼•å…¥äº†è§„åˆ’å™¨çš„æ¦‚å¿µï¼Œå®ƒä½¿ç”¨æç¤ºæ¥è¯·æ±‚ AI é€‰æ‹©è¦è°ƒç”¨çš„å‡½æ•°ã€‚ç„¶è€Œï¼Œè‡ªä»å¼•å…¥Semantic Kernelä»¥æ¥ï¼ŒOpenAI ä¸ºæ¨¡å‹å¼•å…¥äº†ä¸€ç§è°ƒç”¨æˆ–â€œè°ƒç”¨â€å‡½æ•°çš„åŸç”Ÿæ–¹å¼ï¼š[ å‡½æ•°è°ƒç”¨ ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/function-calling/)ã€‚æ­¤åï¼ŒGeminiã€Claude å’Œ Mistral ç­‰å…¶ä»– AI æ¨¡å‹å·²é‡‡ç”¨å‡½æ•°è°ƒç”¨ä½œä¸ºæ ¸å¿ƒåŠŸèƒ½ï¼Œä½¿å…¶æˆä¸ºè·¨æ¨¡å‹æ”¯æŒçš„åŠŸèƒ½ã€‚

Because of these advancements, Semantic Kernel has evolved to use function calling as the primary way to plan and execute tasks.
ç”±äºè¿™äº›è¿›æ­¥ï¼ŒSemantic Kernelå·²ç»å‘å±•åˆ°ä½¿ç”¨å‡½æ•°è°ƒç”¨ä½œä¸ºè§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡çš„ä¸»è¦æ–¹å¼ã€‚

 Important  é‡è¦

Function calling is only available in OpenAI models that are 0613 or newer. If you use an older model (e.g., 0314), this functionality will return an error. We recommend using the latest OpenAI models to take advantage of this feature.
å‡½æ•°è°ƒç”¨ä»…é€‚ç”¨äº 0613 æˆ–æ›´é«˜ç‰ˆæœ¬çš„ OpenAI æ¨¡å‹ã€‚å¦‚æœæ‚¨ä½¿ç”¨è¾ƒæ—§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ 0314ï¼‰ï¼Œæ­¤åŠŸèƒ½å°†è¿”å›é”™è¯¯ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨æœ€æ–°çš„ OpenAI æ¨¡å‹æ¥åˆ©ç”¨æ­¤åŠŸèƒ½ã€‚



## How does function calling create a "plan"? å‡½æ•°è°ƒç”¨å¦‚ä½•åˆ›å»ºâ€œè®¡åˆ’â€ï¼Ÿ

At its simplest, function calling is merely a way for an AI to invoke a function with the right parameters. Take for example a user wants to turn on a light bulb. Assuming the AI has the right plugin, it can call the function to turn on the light.
ç®€å•æ¥è¯´ï¼Œå‡½æ•°è°ƒç”¨åªæ˜¯ AI ä½¿ç”¨æ­£ç¡®å‚æ•°è°ƒç”¨å‡½æ•°çš„ä¸€ç§æ–¹å¼ã€‚ä»¥ç”¨æˆ·æƒ³è¦æ‰“å¼€ç¯æ³¡ä¸ºä¾‹ã€‚å‡è®¾ AI æœ‰æ­£ç¡®çš„æ’ä»¶ï¼Œå®ƒå¯ä»¥è°ƒç”¨å‡½æ•°æ¥æ‰“å¼€ç¯ã€‚

  å±•å¼€è¡¨

| Role  è§’è‰²                                               | Message  æ¶ˆæ¯                                                |
| :------------------------------------------------------- | :----------------------------------------------------------- |
| ğŸ”µ **User**  ğŸ”µ **ç”¨æˆ·**                                   | Please turn on light #1 è¯·æ‰“å¼€ç¯ #1                          |
| ğŸ”´ **Assistant (function call)** ğŸ”´ **åŠ©æ‰‹ ï¼ˆå‡½æ•° è°ƒç”¨ï¼‰** | `Lights.change_state(1, { "isOn": true })`                   |
| ğŸŸ¢ **Tool**  ğŸŸ¢ **å·¥å…·**                                   | `{ "id": 1, "name": "Table Lamp", "isOn": true, "brightness": 100, "hex": "FF0000" }` |
| ğŸ”´ **Assistant**  ğŸ”´ **åŠ©ç†**                              | The lamp is now on ç¯ç°åœ¨äº®äº†                                |

But what if the user doesn't know the ID of the light? Or what if the user wants to turn on all the lights? This is where planning comes in. Today's LLM models are capable of iteratively calling functions to solve a user's need. This is accomplished by creating a feedback loop where the AI can call a function, check the result, and then decide what to do next.
ä½†æ˜¯ï¼Œå¦‚æœç”¨æˆ·ä¸çŸ¥é“ç¯çš„ ID æ€ä¹ˆåŠï¼Ÿæˆ–è€…ï¼Œå¦‚æœç”¨æˆ·æƒ³æ‰“å¼€æ‰€æœ‰ç¯æ€ä¹ˆåŠï¼Ÿè¿™å°±æ˜¯è§„åˆ’çš„ç”¨æ­¦ä¹‹åœ°ã€‚å½“ä»Šçš„ LLM æ¨¡å‹èƒ½å¤Ÿè¿­ä»£è°ƒç”¨å‡½æ•°æ¥è§£å†³ç”¨æˆ·çš„éœ€æ±‚ã€‚è¿™æ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªåé¦ˆå¾ªç¯æ¥å®ç°çš„ï¼Œäººå·¥æ™ºèƒ½å¯ä»¥åœ¨å…¶ä¸­è°ƒç”¨å‡½æ•°ï¼Œæ£€æŸ¥ç»“æœï¼Œç„¶åå†³å®šä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆã€‚

For example, a user may ask the AI to "toggle" a light bulb. The AI would first need to check the state of the light bulb before deciding whether to turn it on or off.
ä¾‹å¦‚ï¼Œç”¨æˆ·å¯èƒ½ä¼šè¦æ±‚äººå·¥æ™ºèƒ½â€œåˆ‡æ¢â€ç¯æ³¡ã€‚äººå·¥æ™ºèƒ½é¦–å…ˆéœ€è¦æ£€æŸ¥ç¯æ³¡çš„çŠ¶æ€ï¼Œç„¶åå†å†³å®šæ˜¯å¦æ‰“å¼€æˆ–å…³é—­å®ƒã€‚

  å±•å¼€è¡¨

| Role  è§’è‰²                                               | Message  æ¶ˆæ¯                                                |
| :------------------------------------------------------- | :----------------------------------------------------------- |
| ğŸ”µ **User**  ğŸ”µ **ç”¨æˆ·**                                   | Please toggle all the lights è¯·åˆ‡æ¢æ‰€æœ‰ç¯                    |
| ğŸ”´ **Assistant (function call)** ğŸ”´ **åŠ©æ‰‹ ï¼ˆå‡½æ•° è°ƒç”¨ï¼‰** | `Lights.get_lights()`                                        |
| ğŸŸ¢ **Tool**  ğŸŸ¢ **å·¥å…·**                                   | `{ "lights": [ { "id": 1, "name": "Table Lamp", "isOn": true, "brightness": 100, "hex": "FF0000" }, { "id": 2, "name": "Ceiling Light", "isOn": false, "brightness": 0, "hex": "FFFFFF" } ] }` |
| ğŸ”´ **Assistant (function call)** ğŸ”´ **åŠ©æ‰‹ ï¼ˆå‡½æ•° è°ƒç”¨ï¼‰** | `Lights.change_state(1, { "isOn": false })` `Lights.change_state(2, { "isOn": true })` |
| ğŸŸ¢ **Tool**  ğŸŸ¢ **å·¥å…·**                                   | `{ "id": 1, "name": "Table Lamp", "isOn": false, "brightness": 0, "hex": "FFFFFF" }` |
| ğŸŸ¢ **Tool**  ğŸŸ¢ **å·¥å…·**                                   | `{ "id": 2, "name": "Ceiling Light", "isOn": true, "brightness": 100, "hex": "FF0000" }` |
| ğŸ”´ **Assistant**  ğŸ”´ **åŠ©ç†**                              | The lights have been toggled ç¯å·²åˆ‡æ¢                        |

 Note  æ³¨æ„

In this example, you also saw parallel function calling. This is where the AI can call multiple functions at the same time. This is a powerful feature that can help the AI solve complex tasks more quickly. It was added to the OpenAI models in 1106.
åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæ‚¨è¿˜çœ‹åˆ°äº†å¹¶è¡Œå‡½æ•°è°ƒç”¨ã€‚è¿™æ˜¯äººå·¥æ™ºèƒ½å¯ä»¥åŒæ—¶è°ƒç”¨å¤šä¸ªå‡½æ•°çš„åœ°æ–¹ã€‚è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŠŸèƒ½ï¼Œå¯ä»¥å¸®åŠ©äººå·¥æ™ºèƒ½æ›´å¿«åœ°è§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚å®ƒäº 1106 å¹´è¢«æ·»åŠ åˆ° OpenAI æ¨¡å‹ä¸­ã€‚



## The automatic planning loop è‡ªåŠ¨è®¡åˆ’å¾ªç¯

Supporting function calling without Semantic Kernel is relatively complex. You would need to write a loop that would accomplish the following:
æ”¯æŒæ²¡æœ‰Semantic Kernelçš„å‡½æ•°è°ƒç”¨ç›¸å¯¹å¤æ‚ã€‚æ‚¨éœ€è¦ç¼–å†™ä¸€ä¸ªå¾ªç¯æ¥å®Œæˆä»¥ä¸‹ä½œï¼š

1. Create JSON schemas for each of your functions
   ä¸ºæ¯ä¸ªå‡½æ•°åˆ›å»º JSON æ¶æ„
2. Provide the LLM with the previous chat history and function schemas
   å‘ LLM æä¾›ä»¥å‰çš„èŠå¤©è®°å½•å’Œå‡½æ•°æ¨¡å¼
3. Parse the LLM's response to determine if it wants to reply with a message or call a function
   è§£æ LLM çš„å“åº”ä»¥ç¡®å®šå®ƒæ˜¯å¦è¦ä½¿ç”¨æ¶ˆæ¯å›å¤æˆ–è°ƒç”¨å‡½æ•°
4. If the LLM wants to call a function, you would need to parse the function name and parameters from the LLM's response
   å¦‚æœ LLM æƒ³è¦è°ƒç”¨å‡½æ•°ï¼Œåˆ™éœ€è¦è§£æ LLM å“åº”ä¸­çš„å‡½æ•°åç§°å’Œå‚æ•°
5. Invoke the function with the right parameters
   ä½¿ç”¨æ­£ç¡®çš„å‚æ•°è°ƒç”¨å‡½æ•°
6. Return the results of the function so that the LLM can determine what it should do next
   è¿”å›å‡½æ•°çš„ç»“æœï¼Œä»¥ä¾¿ LLM å¯ä»¥ç¡®å®šä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆ
7. Repeat steps 2-6 until the LLM decides it has completed the task or needs help from the user
   é‡å¤æ­¥éª¤ 2-6ï¼Œç›´åˆ° LLM ç¡®å®šå·²å®Œæˆä»»åŠ¡æˆ–éœ€è¦ç”¨æˆ·å¸®åŠ©

In Semantic Kernel, we make it easy to use function calling by automating this loop for you. This allows you to focus on building the plugins needed to solve your user's needs.
åœ¨Semantic Kernelä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªåŠ¨æ‰§è¡Œæ­¤å¾ªç¯æ¥ç®€åŒ–å‡½æ•°è°ƒç”¨ã€‚è¿™ä½¿æ‚¨å¯ä»¥ä¸“æ³¨äºæ„å»ºè§£å†³ç”¨æˆ·éœ€æ±‚æ‰€éœ€çš„æ’ä»¶ã€‚

 Note  æ³¨æ„

Understanding how the function calling loop works is essential for building performant and reliable AI agents. For an in-depth look at how the loop works, see the [function calling](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/function-calling/) article.
äº†è§£å‡½æ•°è°ƒç”¨å¾ªç¯çš„å·¥ä½œåŸç†å¯¹äºæ„å»ºé«˜æ€§èƒ½ä¸”å¯é çš„ AI ä»£ç†è‡³å…³é‡è¦ã€‚è¦æ·±å…¥äº†è§£å¾ªç¯çš„å·¥ä½œåŸç†ï¼Œè¯·å‚é˜…[å‡½æ•°è°ƒç”¨](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/function-calling/)ä¸€æ–‡ã€‚



## Using automatic function calling ä½¿ç”¨è‡ªåŠ¨å‡½æ•°è°ƒç”¨

To use automatic function calling in Semantic Kernel, you need to do the following:
è¦åœ¨Semantic Kernelä¸­ä½¿ç”¨è‡ªåŠ¨å‡½æ•°è°ƒç”¨ï¼Œæ‚¨éœ€è¦æ‰§è¡Œä»¥ä¸‹ä½œï¼š

1. Register the plugin with the kernel
   å‘å†…æ ¸æ³¨å†Œæ’ä»¶
2. Create an execution settings object that tells the AI to automatically call functions
   åˆ›å»ºä¸€ä¸ªæ‰§è¡Œè®¾ç½®å¯¹è±¡ï¼Œå‘Šè¯‰ AI è‡ªåŠ¨è°ƒç”¨å‡½æ•°
3. Invoke the chat completion service with the chat history and the kernel
   ä½¿ç”¨èŠå¤©å†å²è®°å½•å’Œå†…æ ¸è°ƒç”¨èŠå¤©å®ŒæˆæœåŠ¡

 Tip  æç¤º

The following code sample uses the `LightsPlugin` defined [here](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins#defining-a-plugin-using-a-class).
ä»¥ä¸‹ä»£ç ç¤ºä¾‹ä½¿ç”¨[æ­¤å¤„](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins#defining-a-plugin-using-a-class)å®šä¹‰çš„ `LightsPlugin`ã€‚

C#Copy  å¤åˆ¶

```csharp
using System.ComponentModel;
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.OpenAI;

// 1. Create the kernel with the Lights plugin
var builder = Kernel.CreateBuilder().AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);
builder.Plugins.AddFromType<LightsPlugin>("Lights");
Kernel kernel = builder.Build();

var chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();

// 2. Enable automatic function calling
OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new() 
{
    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()
};

var history = new ChatHistory();

string? userInput;
do {
    // Collect user input
    Console.Write("User > ");
    userInput = Console.ReadLine();

    // Add user input
    history.AddUserMessage(userInput);

    // 3. Get the response from the AI with automatic function calling
    var result = await chatCompletionService.GetChatMessageContentAsync(
        history,
        executionSettings: openAIPromptExecutionSettings,
        kernel: kernel);

    // Print the results
    Console.WriteLine("Assistant > " + result);

    // Add the message from the agent to the chat history
    history.AddMessage(result.Role, result.Content ?? string.Empty);
} while (userInput is not null)
```

When you use automatic function calling, all of the steps in the automatic planning loop are handled for you and added to the `ChatHistory` object. After the function calling loop is complete, you can inspect the `ChatHistory` object to see all of the function calls made and results provided by Semantic Kernel.
ä½¿ç”¨è‡ªåŠ¨å‡½æ•°è°ƒç”¨æ—¶ï¼Œç³»ç»Ÿä¼šä¸ºä½ å¤„ç†è‡ªåŠ¨è®¡åˆ’å¾ªç¯ä¸­çš„æ‰€æœ‰æ­¥éª¤ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ° `ChatHistory` å¯¹è±¡ä¸­ã€‚å‡½æ•°è°ƒç”¨å¾ªç¯å®Œæˆåï¼Œå¯ä»¥æ£€æŸ¥ `ChatHistory` å¯¹è±¡ï¼ŒæŸ¥çœ‹Semantic Kernelæä¾›çš„æ‰€æœ‰å‡½æ•°è°ƒç”¨å’Œç»“æœã€‚



## What happened to the Stepwise and Handlebars planners? Stepwise å’Œ Handlebars è§„åˆ’å™¨æ€ä¹ˆäº†ï¼Ÿ

The Stepwise and Handlebars planners have been deprecated and removed from the Semantic Kernel package. These planners are no longer supported in either Python, .NET, or Java.
Stepwise å’Œ Handlebars è§„åˆ’å™¨å·²è¢«å¼ƒç”¨ï¼Œå¹¶ä» Semantic Kernel åŒ…ä¸­åˆ é™¤ã€‚Pythonã€.NET æˆ– Java ä¸å†æ”¯æŒè¿™äº›è§„åˆ’å™¨ã€‚

We recommend using **function calling**, which is both more powerful and easier to use for most scenarios.
å»ºè®®ä½¿ç”¨**å‡½æ•°è°ƒç”¨** ï¼Œå¯¹äºå¤§å¤šæ•°æ–¹æ¡ˆï¼Œå‡½æ•°è°ƒç”¨åŠŸèƒ½æ›´å¼ºå¤§ä¸”æ›´æ˜“äºä½¿ç”¨ã€‚

To update existing solutions, follow our [Stepwise Planner Migration Guide](https://learn.microsoft.com/en-us/semantic-kernel/support/migration/stepwise-planner-migration-guide).
è¦æ›´æ–°ç°æœ‰è§£å†³æ–¹æ¡ˆï¼Œè¯·æŒ‰ç…§æˆ‘ä»¬çš„ [Stepwise Planner è¿ç§»æŒ‡å—](https://learn.microsoft.com/en-us/semantic-kernel/support/migration/stepwise-planner-migration-guide)è¿›è¡Œä½œã€‚

 Tip  æç¤º

For new AI agents, use function calling instead of the deprecated planners. It offers better flexibility, built-in tool support, and a simpler development experience.
å¯¹äºæ–°çš„ AI ä»£ç†ï¼Œè¯·ä½¿ç”¨å‡½æ•°è°ƒç”¨è€Œä¸æ˜¯å·²å¼ƒç”¨çš„è§„åˆ’å™¨ã€‚å®ƒæä¾›äº†æ›´å¥½çš„çµæ´»æ€§ã€å†…ç½®çš„å·¥å…·æ”¯æŒå’Œæ›´ç®€å•çš„å¼€å‘ä½“éªŒã€‚



## Next steps  åç»­æ­¥éª¤

Now that you understand how planners work in Semantic Kernel, you can learn more about how influence your AI agent so that they best plan and execute tasks on behalf of your users.
ç°åœ¨æ‚¨å·²ç»äº†è§£äº†è§„åˆ’å™¨åœ¨Semantic Kernelä¸­çš„å·¥ä½œåŸç†ï¼Œæ‚¨å¯ä»¥è¯¦ç»†äº†è§£å¦‚ä½•å½±å“æ‚¨çš„ AI ä»£ç†ï¼Œä»¥ä¾¿ä»–ä»¬ä»£è¡¨æ‚¨çš„ç”¨æˆ·æœ€å¥½åœ°è®¡åˆ’å’Œæ‰§è¡Œä»»åŠ¡ã€‚