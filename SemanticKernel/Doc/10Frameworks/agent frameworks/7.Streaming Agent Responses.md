# How to Stream Agent Responses 如何流式传输代理响应

- 05/23/2025



## What is a Streamed Response? 什么是流式响应？

A streamed response delivers the message content in small, incremental chunks. This approach enhances the user experience by allowing them to view and engage with the message as it unfolds, rather than waiting for the entire response to load. Users can begin processing information immediately, improving the sense of responsiveness and interactivity. As a result, it minimizes delays and keeps users more engaged throughout the communication process.
流式响应以较小的增量块传递消息内容。这种方法允许他们在消息展开时查看和参与消息，而不是等待整个响应加载，从而增强用户体验。用户可以立即开始处理信息，提高响应感和交互性。因此，它可以最大限度地减少延迟并让用户在整个通信过程中更加参与。



### Streaming References  流式引用

- [OpenAI Streaming Guide  OpenAI 流媒体指南](https://platform.openai.com/docs/api-reference/streaming)
- [OpenAI Chat Completion Streaming
  OpenAI 聊天完成流](https://platform.openai.com/docs/api-reference/chat/create#chat-create-stream)
- [OpenAI Assistant Streaming
  OpenAI 助手流媒体](https://platform.openai.com/docs/api-reference/assistants-streaming)
- [Azure OpenAI Service REST API
  Azure OpenAI 服务 REST API](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)



## Streaming in Semantic Kernel Semantic Kernel中的流式传输

[AI Services](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/) that support streaming in Semantic Kernel use different content types compared to those used for fully-formed messages. These content types are specifically designed to handle the incremental nature of streaming data. The same content types are also utilized within the Agent Framework for similar purposes. This ensures consistency and efficiency across both systems when dealing with streaming information.
与用于完整格式消息的内容类型相比，支持Semantic Kernel流式处理的 [AI 服务](https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/)使用不同的内容类型。这些内容类型专门设计用于处理流数据的增量性质。代理框架中也将相同的内容类型用于类似的目的。这确保了在处理流信息时两个系统之间的一致性和效率。

 Tip  提示

API reference:  API 参考：

- [`StreamingChatMessageContent`](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.streamingchatmessagecontent)
- [`StreamingTextContent`](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.streamingtextcontent)
- [`StreamingFileReferenceContent`](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.streamingfilereferencecontent)
- [`StreamingAnnotationContent`](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.agents.openai.streamingannotationcontent)



### Streamed response from `ChatCompletionAgent` 来自 `ChatCompletionAgent` 的流式响应

When invoking a streamed response from a [`ChatCompletionAgent`](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/chat-completion-agent), the `ChatHistory` in the `AgentThread` is updated after the full response is received. Although the response is streamed incrementally, the history records only the complete message. This ensures that the `ChatHistory` reflects fully formed responses for consistency.
从 [`ChatCompletionAgent`](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/chat-completion-agent) 调用流式响应时，`AgentThread` 中的 `ChatHistory` 会在收到完整响应后更新。尽管响应是以增量方式流式传输的，但历史记录仅记录完整的消息。这可确保 `ChatHistory` 反映完整格式的响应以保持一致性。

C#Copy  复制

```csharp
// Define agent
ChatCompletionAgent agent = ...;

ChatHistoryAgentThread agentThread = new();

// Create a user message
var message = ChatMessageContent(AuthorRole.User, "<user input>");

// Generate the streamed agent response(s)
await foreach (StreamingChatMessageContent response in agent.InvokeStreamingAsync(message, agentThread))
{
  // Process streamed response(s)...
}

// It's also possible to read the messages that were added to the ChatHistoryAgentThread.
await foreach (ChatMessageContent response in agentThread.GetMessagesAsync())
{
  // Process messages...
}
```



### Streamed response from `OpenAIAssistantAgent` 来自 `OpenAIAssistantAgent` 的流式响应

When invoking a streamed response from an [`OpenAIAssistantAgent`](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/assistant-agent), the assistant maintains the conversation state as a remote thread. It is possible to read the messages from the remote thread if required.
从 [`OpenAIAssistantAgent`](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/assistant-agent) 调用流式响应时，助手将对话状态维护为远程线程。如果需要，可以从远程线程读取消息。

C#Copy  复制

```csharp
// Define agent
OpenAIAssistantAgent agent = ...;

// Create a thread for the agent conversation.
OpenAIAssistantAgentThread agentThread = new(assistantClient);

// Create a user message
var message = new ChatMessageContent(AuthorRole.User, "<user input>");

// Generate the streamed agent response(s)
await foreach (StreamingChatMessageContent response in agent.InvokeStreamingAsync(message, agentThread))
{
  // Process streamed response(s)...
}

// It's possible to read the messages from the remote thread.
await foreach (ChatMessageContent response in agentThread.GetMessagesAsync())
{
  // Process messages...
}

// Delete the thread when it is no longer needed
await agentThread.DeleteAsync();
```

To create a thread using an existing `Id`, pass it to the constructor of `OpenAIAssistantAgentThread`:
要使用现有 `Id` 创建线程，请将其传递给 `OpenAIAssistantAgentThread` 的构造函数：

C#Copy  复制

```csharp
// Define agent
OpenAIAssistantAgent agent = ...;

// Create a thread for the agent conversation.
OpenAIAssistantAgentThread agentThread = new(assistantClient, "your-existing-thread-id");

// Create a user message
var message = new ChatMessageContent(AuthorRole.User, "<user input>");

// Generate the streamed agent response(s)
await foreach (StreamingChatMessageContent response in agent.InvokeStreamingAsync(message, agentThread))
{
  // Process streamed response(s)...
}

// It's possible to read the messages from the remote thread.
await foreach (ChatMessageContent response in agentThread.GetMessagesAsync())
{
  // Process messages...
}

// Delete the thread when it is no longer needed
await agentThread.DeleteAsync();
```



## Handling Intermediate Messages with a Streaming Response 使用流式处理响应的中间消息

The nature of streaming responses allows LLM models to return incremental chunks of text, enabling quicker rendering in a UI or console without waiting for the entire response to complete. Additionally, a caller might want to handle intermediate content, such as results from function calls. This can be achieved by supplying a callback function when invoking the streaming response. The callback function receives complete messages encapsulated within `ChatMessageContent`.
流式响应的性质允许 LLM 模型返回增量文本块，从而在 UI 或控制台中更快地呈现，而无需等待整个响应完成。此外，调用方可能想要处理中间内容，例如函数调用的结果。这可以通过在调用流式处理响应时提供回调函数来实现。回调函数接收封装在 `ChatMessageContent` 中的完整消息。

> Callback documentation for the `AzureAIAgent` is coming soon.
> `AzureAIAgent` 的回调文档即将推出。



## Next Steps  后续步骤

[  将模板与代理一起使用](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-templates)

[  代理业务流程](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-orchestration/)