# What is a Plugin?  什么是插件？

- 12/11/2024

Plugins are a key component of Semantic Kernel. If you have already used plugins from ChatGPT or Copilot extensions in Microsoft 365, you’re already familiar with them. With plugins, you can encapsulate your existing APIs into a collection that can be used by an AI. This allows you to give your AI the ability to perform actions that it wouldn’t be able to do otherwise.
插件是Semantic Kernel的关键组件。如果您已经在 Microsoft 365 中使用过 ChatGPT 或 Copilot 扩展中的插件，那么您已经熟悉它们。使用插件，您可以将现有 API 封装到可供 AI 使用的集合中。这使您可以让您的 AI 能够执行其他方式无法执行的作。

Behind the scenes, Semantic Kernel leverages [function calling](https://platform.openai.com/docs/guides/function-calling), a native feature of most of the latest LLMs to allow LLMs, to perform [planning](https://learn.microsoft.com/en-us/semantic-kernel/concepts/planning) and to invoke your APIs. With function calling, LLMs can request (i.e., call) a particular function. Semantic Kernel then marshals the request to the appropriate function in your codebase and returns the results back to the LLM so the LLM can generate a final response.
在幕后，Semantic Kernel利用[函数调用 ](https://platform.openai.com/docs/guides/function-calling)（大多数最新 LLM 的原生功能）来允许 LLM 执行[规划](https://learn.microsoft.com/en-us/semantic-kernel/concepts/planning)和调用 API。通过函数调用，LLM 可以请求（即调用）特定函数。然后，Semantic Kernel将请求编组到代码库中的相应函数，并将结果返回给 LLM，以便 LLM 可以生成最终响应。

![Semantic Kernel Plugin](https://learn.microsoft.com/en-us/semantic-kernel/media/designed-for-modular-extensibility-vertical.png)

Not all AI SDKs have an analogous concept to plugins (most just have functions or tools). In enterprise scenarios, however, plugins are valuable because they encapsulate a set of functionality that mirrors how enterprise developers already develop services and APIs. Plugins also play nicely with dependency injection. Within a plugin's constructor, you can inject services that are necessary to perform the work of the plugin (e.g., database connections, HTTP clients, etc.). This is difficult to accomplish with other SDKs that lack plugins.
并非所有 AI SDK 都有与插件类似的概念（大多数只是具有函数或工具）。然而，在企业场景中，插件很有价值，因为它们封装了一组功能，反映了企业开发人员已经开发服务和 API 的方式。插件也可以很好地与依赖注入配合使用。在插件的构造函数中，您可以注入执行插件工作所需的服务（例如，数据库连接、HTTP 客户端等）。这对于其他缺少插件的 SDK 来说是很难实现的。



## Anatomy of a plugin  插件剖析

At a high-level, a plugin is a group of [functions](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#importing-different-types-of-plugins) that can be exposed to AI apps and services. The functions within plugins can then be orchestrated by an AI application to accomplish user requests. Within Semantic Kernel, you can invoke these functions automatically with function calling.
从高层次上讲，插件是一组可以向 AI 应用程序和服务[公开的功能。](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#importing-different-types-of-plugins) 然后，插件中的函数可以由 AI 应用程序编排以完成用户请求。在Semantic Kernel中，您可以通过函数调用自动调用这些函数。

 Note  注意

In other platforms, functions are often referred to as "tools" or "actions". In Semantic Kernel, we use the term "functions" since they are typically defined as native functions in your codebase.
在其他平台中，函数通常被称为“工具”或“作”。在Semantic Kernel中，我们使用术语“函数”，因为它们通常在代码库中定义为本机函数。

Just providing functions, however, is not enough to make a plugin. To power automatic orchestration with function calling, plugins also need to provide details that semantically describe how they behave. Everything from the function's input, output, and side effects need to be described in a way that the AI can understand, otherwise, the AI will not correctly call the function.
然而，仅仅提供功能不足以制作插件。为了通过函数调用为自动编排提供支持，插件还需要提供语义上描述其行为方式的详细信息。从函数的输入、输出到副作用，一切都需要以 AI 可以理解的方式进行描述，否则，AI 将无法正确调用函数。

For example, the sample `WriterPlugin` plugin on the right has functions with semantic descriptions that describe what each function does. An LLM can then use these descriptions to choose the best functions to call to fulfill a user's ask.
例如，右侧的示例 `WriterPlugin` 插件具有语义描述的函数，这些描述描述每个函数的作用。然后，LLM 可以使用这些描述来选择要调用的最佳函数来满足用户的要求。

In the picture on the right, an LLM would likely call the `ShortPoem` and `StoryGen` functions to satisfy the users ask thanks to the provided semantic descriptions.
在右图中，LLM 可能会调用 `ShortPoem` 和 `StoryGen` 函数来满足用户对提供的语义描述的要求。

![Semantic description within the WriterPlugin plugin](https://learn.microsoft.com/en-us/semantic-kernel/media/writer-plugin-example.png)



### Importing different types of plugins 导入不同类型的插件

There are three primary ways of importing plugins into Semantic Kernel: using [native code](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins), using an [OpenAPI specification](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins) or from a [MCP Server](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-mcp-plugins) The former allows you to author plugins in your existing codebase that can leverage dependencies and services you already have. The latter two allow you to import plugins from an OpenAPI specification or a MCP Server, which can be shared across different programming languages and platforms.
将插件导入Semantic Kernel有三种主要方法：使用[本机代码 ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins)、使用 [OpenAPI 规范](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins)或从 [MCP 服务器](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-mcp-plugins)前者允许您在现有代码库中创作插件，这些插件可以利用您已有的依赖项和服务。后两者允许您从 OpenAPI 规范或 MCP 服务器导入插件，这些插件可以在不同的编程语言和平台之间共享。

Below we provide a simple example of importing and using a native plugin. To learn more about how to import these different types of plugins, refer to the following articles:
下面我们提供了一个导入和使用本机插件的简单示例。要了解有关如何导入这些不同类型的插件的更多信息，请参阅以下文章：

- [Importing native code  导入本机代码](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins)
- [Importing an OpenAPI specification
  导入 OpenAPI 规范](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins)
- [Importing a MCP Server
  导入 MCP 服务器](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-mcp-plugins)

 Tip  提示

When getting started, we recommend using native code plugins. As your application matures, and as you work across cross-platform teams, you may want to consider using OpenAPI specifications to share plugins across different programming languages and platforms. You can then also create a MCP Server from your Kernel instance, which allows other applications to consume your plugins as a service.
开始时，我们建议使用本机代码插件。随着应用程序的成熟，以及跨平台团队的工作，您可能需要考虑使用 OpenAPI 规范在不同的编程语言和平台上共享插件。然后，您还可以从内核实例创建 MCP 服务器，这允许其他应用程序将您的插件作为服务使用。



### The different types of plugin functions 不同类型的插件功能

Within a plugin, you will typically have two different types of functions, those that retrieve data for retrieval augmented generation (RAG) and those that automate tasks. While each type is functionally the same, they are typically used differently within applications that use Semantic Kernel.
在插件中，您通常会有两种不同类型的函数，一种是检索数据以进行检索增强生成 （RAG） 的函数，另一种是自动执行任务的函数。虽然每种类型在功能上都相同，但它们在使用Semantic Kernel的应用程序中的使用方式通常不同。

For example, with retrieval functions, you may want to use strategies to improve performance (e.g., caching and using cheaper intermediate models for summarization). Whereas with task automation functions, you'll likely want to implement human-in-the-loop approval processes to ensure that tasks are completed correctly.
例如，对于检索函数，您可能希望使用策略来提高性能（例如，缓存和使用更便宜的中间模型进行汇总）。而对于任务自动化功能，您可能需要实施人机交互审批流程，以确保任务正确完成。

To learn more about the different types of plugin functions, refer to the following articles:
要了解有关不同类型的插件功能的更多信息，请参阅以下文章：

- [Data retrieval functions
  数据检索功能](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/using-data-retrieval-functions-for-rag)
- [Task automation functions
  任务自动化功能](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/using-task-automation-functions)



## Getting started with plugins 插件入门

Using plugins within Semantic Kernel is always a three step process:
在Semantic Kernel中使用插件始终是一个三步过程：

1. [Define your plugin  定义插件](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#1-define-your-plugin)
2. [Add the plugin to your kernel
   将插件添加到内核](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#2-add-the-plugin-to-your-kernel)
3. [And then either invoke the plugin's functions in either a prompt with function calling
   然后在带有函数调用的提示中调用插件的函数](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#3-invoke-the-plugins-functions)

Below we'll provide a high-level example of how to use a plugin within Semantic Kernel. Refer to the links above for more detailed information on how to create and use plugins.
下面我们将提供如何在Semantic Kernel中使用插件的高级示例。有关如何创建和使用插件的更多详细信息，请参阅上面的链接。



### 1) Define your plugin  1） 定义你的插件

The easiest way to create a plugin is by defining a class and annotating its methods with the `KernelFunction` attribute. This let's Semantic Kernel know that this is a function that can be called by an AI or referenced in a prompt.
创建插件的最简单方法是定义一个类并使用 `KernelFunction` 属性注释其方法。这让Semantic Kernel知道这是一个可以由 AI 调用或在提示中引用的函数。

You can also import plugins from an [OpenAPI specification](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins).
您还可以从 [OpenAPI 规范](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins)导入插件。

Below, we'll create a plugin that can retrieve the state of lights and alter its state.
下面，我们将创建一个插件，可以检索光源的状态并更改其状态。

 Tip  提示

Since most LLM have been trained with Python for function calling, its recommended to use snake case for function names and property names even if you're using the C# or Java SDK.
由于大多数 LLM 都已使用 Python 进行函数调用训练，因此即使您使用的是 C# 或 Java SDK，也建议使用蛇形命名法作为函数名称和属性名称。

C#Copy  复制

```csharp
using System.ComponentModel;
using Microsoft.SemanticKernel;

public class LightsPlugin
{
   // Mock data for the lights
   private readonly List<LightModel> lights = new()
   {
      new LightModel { Id = 1, Name = "Table Lamp", IsOn = false, Brightness = 100, Hex = "FF0000" },
      new LightModel { Id = 2, Name = "Porch light", IsOn = false, Brightness = 50, Hex = "00FF00" },
      new LightModel { Id = 3, Name = "Chandelier", IsOn = true, Brightness = 75, Hex = "0000FF" }
   };

   [KernelFunction("get_lights")]
   [Description("Gets a list of lights and their current state")]
   public async Task<List<LightModel>> GetLightsAsync()
   {
      return lights
   }

   [KernelFunction("get_state")]
   [Description("Gets the state of a particular light")]
   public async Task<LightModel?> GetStateAsync([Description("The ID of the light")] int id)
   {
      // Get the state of the light with the specified ID
      return lights.FirstOrDefault(light => light.Id == id);
   }

   [KernelFunction("change_state")]
   [Description("Changes the state of the light")]
   public async Task<LightModel?> ChangeStateAsync(int id, LightModel LightModel)
   {
      var light = lights.FirstOrDefault(light => light.Id == id);

      if (light == null)
      {
         return null;
      }

      // Update the light with the new state
      light.IsOn = LightModel.IsOn;
      light.Brightness = LightModel.Brightness;
      light.Hex = LightModel.Hex;

      return light;
   }
}

public class LightModel
{
   [JsonPropertyName("id")]
   public int Id { get; set; }

   [JsonPropertyName("name")]
   public string Name { get; set; }

   [JsonPropertyName("is_on")]
   public bool? IsOn { get; set; }

   [JsonPropertyName("brightness")]
   public byte? Brightness { get; set; }

   [JsonPropertyName("hex")]
   public string? Hex { get; set; }
}
```

Notice that we provide descriptions for the function, and parameters. This is important for the AI to understand what the function does and how to use it.
请注意，我们提供了函数和参数的描述。这对于人工智能了解该功能的作用以及如何使用它非常重要。

 Tip  提示

Don't be afraid to provide detailed descriptions for your functions if an AI is having trouble calling them. Few-shot examples, recommendations for when to use (and not use) the function, and guidance on where to get required parameters can all be helpful.
如果 AI 在调用函数时遇到问题，请不要害怕为您的函数提供详细描述。少量示例、何时使用（和不使用）函数的建议以及有关从何处获取所需参数的指南都会有所帮助。



### 2) Add the plugin to your kernel 2） 将插件添加到您的内核中

Once you've defined your plugin, you can add it to your kernel by creating a new instance of the plugin and adding it to the kernel's plugin collection.
定义插件后，您可以通过创建插件的新实例并将其添加到内核的插件集合中来将其添加到内核中。

This example demonstrates the easiest way of adding a class as a plugin with the `AddFromType` method. To learn about other ways of adding plugins, refer to the [adding native plugins](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins) article.
此示例演示了使用 `AddFromType` 方法将类添加为插件的最简单方法。要了解添加插件的其他方法，请参阅[添加本机插件](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins)一文。

C#Copy  复制

```csharp
var builder = new KernelBuilder();
builder.Plugins.AddFromType<LightsPlugin>("Lights")
Kernel kernel = builder.Build();
```



### 3) Invoke the plugin's functions 3）调用插件的功能

Finally, you can have the AI invoke your plugin's functions by using function calling. Below is an example that demonstrates how to coax the AI to call the `get_lights` function from the `Lights` plugin before calling the `change_state` function to turn on a light.
最后，你可以让 AI 使用函数调用来调用插件的函数。下面是一个示例，演示了如何在调用 `change_state` 函数打开灯源之前，诱使 AI 从 `Lights` 插件调用 `get_lights` 函数。

C#Copy  复制

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.OpenAI;

// Create a kernel with Azure OpenAI chat completion
var builder = Kernel.CreateBuilder().AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);

// Build the kernel
Kernel kernel = builder.Build();
var chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();

// Add a plugin (the LightsPlugin class is defined below)
kernel.Plugins.AddFromType<LightsPlugin>("Lights");

// Enable planning
OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new() 
{
    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()
};

// Create a history store the conversation
var history = new ChatHistory();
history.AddUserMessage("Please turn on the lamp");

// Get the response from the AI
var result = await chatCompletionService.GetChatMessageContentAsync(
   history,
   executionSettings: openAIPromptExecutionSettings,
   kernel: kernel);

// Print the results
Console.WriteLine("Assistant > " + result);

// Add the message from the agent to the chat history
history.AddAssistantMessage(result);
```

With the above code, you should get a response that looks like the following:
使用上面的代码，您应该会得到如下所示的响应：

  展开表

| Role  角色                                               | Message  消息                                                |
| :------------------------------------------------------- | :----------------------------------------------------------- |
| 🔵 **User**  🔵 **用户**                                   | Please turn on the lamp 请打开 lamp                          |
| 🔴 **Assistant (function call)** 🔴 **助手 （函数 调用）** | `Lights.get_lights()`                                        |
| 🟢 **Tool**  🟢 **工具**                                   | `[{ "id": 1, "name": "Table Lamp", "isOn": false, "brightness": 100, "hex": "FF0000" }, { "id": 2, "name": "Porch light", "isOn": false, "brightness": 50, "hex": "00FF00" }, { "id": 3, "name": "Chandelier", "isOn": true, "brightness": 75, "hex": "0000FF" }]` |
| 🔴 **Assistant (function call)** 🔴 **助手 （函数 调用）** | Lights.change_state(1, { "isOn": true }) Lights.change_state（1， { “isOn”： true }） |
| 🟢 **Tool**  🟢 **工具**                                   | `{ "id": 1, "name": "Table Lamp", "isOn": true, "brightness": 100, "hex": "FF0000" }` |
| 🔴 **Assistant**  🔴 **助理**                              | The lamp is now on 灯现在亮了                                |

 Tip  提示

While you can invoke a plugin function directly, this is not advised because the AI should be the one deciding which functions to call. If you need explicit control over which functions are called, consider using standard methods in your codebase instead of plugins.
虽然你可以直接调用插件函数，但不建议这样做，因为 AI 应该决定要调用哪些函数。如果您需要显式控制调用哪些函数，请考虑在代码库中使用标准方法而不是插件。



## General recommendations for authoring plugins 创作插件的一般建议

Considering that each scenario has unique requirements, utilizes distinct plugin designs, and may incorporate multiple LLMs, it is challenging to provide a one-size-fits-all guide for plugin design. However, below are some general recommendations and guidelines to ensure that plugins are AI-friendly and can be easily and efficiently consumed by LLMs.
考虑到每个场景都有独特的要求，使用不同的插件设计，并且可能包含多个 LLM，因此为插件设计提供一刀切的指南具有挑战性。但是，以下是一些一般建议和指南，以确保插件对 AI 友好并且 LLM 可以轻松高效地使用。



### Import only the necessary plugins 仅导入必要的插件

Import only the plugins that contain functions necessary for your specific scenario. This approach will not only reduce the number of input tokens consumed but also minimize the occurrence of function miscalls-calls to functions that are not used in the scenario. Overall, this strategy should enhance function-calling accuracy and decrease the number of false positives.
仅导入包含特定场景所需功能的插件。这种方法不仅可以减少消耗的输入令牌数，还可以最大限度地减少函数错误调用的发生 - 对方案中未使用的函数的调用。总体而言，此策略应提高函数调用准确性并减少误报数量。

Additionally, OpenAI recommends that you use no more than 20 tools in a single API call; ideally, no more than 10 tools. As stated by OpenAI: *"We recommend that you use no more than 20 tools in a single API call. Developers typically see a reduction in the model's ability to select the correct tool once they have between 10-20 tools defined."** For more information, you can visit their documentation at [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling#keep-the-number-of-functions-low-for-higher-accuracy).
此外，OpenAI 建议您在单个 API 调用中使用不超过 20 个工具;理想情况下，不超过 10 个工具。正如 OpenAI 所说：“ *我们建议您在单个 API 调用中使用不超过 20 个工具。一旦开发人员定义了 10-20 个工具，他们通常会看到模型选择正确工具的能力下降。** 有关更多信息，您可以访问他们的文档，网址为 [OpenAI 函数调用指南 ](https://platform.openai.com/docs/guides/function-calling#keep-the-number-of-functions-low-for-higher-accuracy)。



### Make plugins AI-friendly  使插件对人工智能友好

To enhance the LLM's ability to understand and utilize plugins, it is recommended to follow these guidelines:
为了增强 LLM 理解和使用插件的能力，建议遵循以下准则：

- **Use descriptive and concise function names:** Ensure that function names clearly convey their purpose to help the model understand when to select each function. If a function name is ambiguous, consider renaming it for clarity. Avoid using abbreviations or acronyms to shorten function names. Utilize the `DescriptionAttribute` to provide additional context and instructions only when necessary, minimizing token consumption.
  **使用描述性和简洁的函数名称：** 确保函数名称清楚地传达其用途，以帮助模型了解何时选择每个函数。如果函数名称不明确，请考虑重命名以清楚起见。避免使用缩写或首字母缩略词来缩短函数名称。仅在必要时利用 `DescriptionAttribute` 提供额外的上下文和说明，从而最大限度地减少令牌消耗。
- **Minimize function parameters:** Limit the number of function parameters and use primitive types whenever possible. This approach reduces token consumption and simplifies the function signature, making it easier for the LLM to match function parameters effectively.
  **最小化函数参数：** 限制函数参数的数量，并尽可能使用基元类型。这种方法减少了令牌消耗并简化了函数签名，使 LLM 更容易有效地匹配函数参数。
- **Name function parameters clearly:** Assign descriptive names to function parameters to clarify their purpose. Avoid using abbreviations or acronyms to shorten parameter names, as this will assist the LLM in reasoning about the parameters and providing accurate values. As with function names, use the `DescriptionAttribute` only when necessary to minimize token consumption.
  **明确命名函数参数：** 为函数参数分配描述性名称以阐明其用途。避免使用缩写或首字母缩略词来缩短参数名称，因为这将有助于 LLM 推理参数并提供准确的值。与函数名称一样，仅在必要时使用 `DescriptionAttribute` 以最大程度地减少令牌消耗。



### Find a right balance between the number of functions and their responsibilities 在职能数量及其职责之间找到适当的平衡

On one hand, having functions with a single responsibility is a good practice that allows to keep functions simple and reusable across multiple scenarios. On the other hand, each function call incurs overhead in terms of network round-trip latency and the number of consumed input and output tokens: input tokens are used to send the function definition and invocation result to the LLM, while output tokens are consumed when receiving the function call from the model. Alternatively, a single function with multiple responsibilities can be implemented to reduce the number of consumed tokens and lower network overhead, although this comes at the cost of reduced reusability in other scenarios.
一方面，将函数具有单一职责是一种很好的做法，它允许使函数保持简单且可在多个方案中重用。另一方面，每次函数调用在网络往返延迟和消耗的输入和输出 token 数量方面都会产生开销：输入 token 用于将函数定义和调用结果发送给 LLM，而输出 token 则在接收模型的函数调用时消耗。或者，可以实现具有多个职责的单个功能，以减少消耗的代币数量并降低网络开销，尽管这是以降低其他场景的可重用性为代价的。

However, consolidating many responsibilities into a single function may increase the number and complexity of function parameters and its return type. This complexity can lead to situations where the model may struggle to correctly match the function parameters, resulting in missed parameters or values of incorrect type. Therefore, it is essential to strike the right balance between the number of functions to reduce network overhead and the number of responsibilities each function has, ensuring that the model can accurately match function parameters.
但是，将许多职责合并到单个函数中可能会增加函数参数及其返回类型的数量和复杂性。这种复杂性可能导致模型可能难以正确匹配函数参数，从而导致遗漏参数或类型不正确的值。因此，必须在减少网络开销的函数数量和每个函数的职责数量之间取得适当的平衡，确保模型能够准确匹配函数参数。



### Transform Semantic Kernel functions 转换Semantic Kernel函数

Utilize the transformation techniques for Semantic Kernel functions as described in the [Transforming Semantic Kernel Functions](https://devblogs.microsoft.com/semantic-kernel/transforming-semantic-kernel-functions/#:~:text=Semantic Kernel provides a series,from an Open API specification) blog post to:
利用Semantic Kernel函数的转换技术，如转换语[义内核函数](https://devblogs.microsoft.com/semantic-kernel/transforming-semantic-kernel-functions/#:~:text=Semantic Kernel provides a series,from an Open API specification)博客文章中所述，可以：

- **Change function behavior:** There are scenarios where the default behavior of a function may not align with the desired outcome and it's not feasible to modify the original function's implementation. In such cases, you can create a new function that wraps the original one and modifies its behavior accordingly.
  **更改功能行为：** 在某些情况下，函数的默认行为可能与预期结果不一致，并且修改原始函数的实现是不可行的。在这种情况下，您可以创建一个新函数来包装原始函数并相应地修改其行为。
- **Provide context information:** Functions may require parameters that the LLM cannot or should not infer. For example, if a function needs to act on behalf of the current user or requires authentication information, this context is typically available to the host application but not to the LLM. In such cases, you can transform the function to invoke the original one while supplying the necessary context information from the hosting application, along with arguments provided by the LLM.
  **提供上下文信息：** 函数可能需要 LLM 无法或不应该推断的参数。例如，如果函数需要代表当前用户执行作或需要身份验证信息，则此上下文通常可供主机应用程序使用，但不适用于 LLM。在这种情况下，您可以转换函数以调用原始函数，同时从托管应用程序提供必要的上下文信息以及 LLM 提供的参数。
- **Change parameters list, types, and names:** If the original function has a complex signature that the LLM struggles to interpret, you can transform the function into one with a simpler signature that the LLM can more easily understand. This may involve changing parameter names, types, the number of parameters, and flattening or unflattening complex parameters, among other adjustments.
  **更改参数列表、类型和名称：** 如果原始函数具有 LLM 难以解释的复杂签名，您可以将该函数转换为具有 LLM 更容易理解的更简单签名的函数。这可能涉及更改参数名称、类型、参数数量以及展平或取消展平复杂参数等调整。



### Local state utilization  本地状态利用率

When designing plugins that operate on relatively large or confidential datasets, such as documents, articles, or emails containing sensitive information, consider utilizing local state to store original data or intermediate results that do not need to be sent to the LLM. Functions for such scenarios can accept and return a state id, allowing you to look up and access the data locally instead of passing the actual data to the LLM, only to receive it back as an argument for the next function invocation.
在设计对相对较大或机密数据集（例如包含敏感信息的文档、文章或电子邮件）进行作的插件时，请考虑利用本地状态来存储不需要发送到 LLM 的原始数据或中间结果。此类场景的函数可以接受并返回状态 ID，允许您在本地查找和访问数据，而不是将实际数据传递给 LLM，只是将其作为下一个函数调用的参数接收回来。

By storing data locally, you can keep the information private and secure while avoiding unnecessary token consumption during function calls. This approach not only enhances data privacy but also improves overall efficiency in processing large or sensitive datasets.
通过在本地存储数据，可以保持信息的私密性和安全性，同时避免在函数调用期间不必要的令牌消耗。这种方法不仅增强了数据隐私，还提高了处理大型或敏感数据集的整体效率。



### Provide function return type schema to AI model 向 AI 模型提供函数返回类型架构

Use one of the techniques described in the [Providing functions return type schema to LLM](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins#provide-function-return-type-information-in-function-description) section to provide the function's return type schema to the AI model.
使用 向 [LLM 提供函数返回类型架构 ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins#provide-function-return-type-information-in-function-description)部分中所述的技术之一，向 AI 模型提供函数的返回类型架构。

By utilizing a well-defined return type schema, the AI model can accurately identify the intended properties, eliminating potential inaccuracies that may arise when the model makes assumptions based on incomplete or ambiguous information in the absence of the schema. Consequently, this enhances the accuracy of function calls, leading to more reliable and precise outcomes.
通过利用定义明确的返回类型模式，人工智能模型可以准确识别预期属性，消除模型在没有模式的情况下根据不完整或模棱两可的信息做出假设时可能出现的潜在不准确之处。因此，这提高了函数调用的准确性，从而获得更可靠和精确的结果。