# What is a Plugin?  ä»€ä¹ˆæ˜¯æ’ä»¶ï¼Ÿ

- 12/11/2024

Plugins are a key component of Semantic Kernel. If you have already used plugins from ChatGPT or Copilot extensions in Microsoft 365, youâ€™re already familiar with them. With plugins, you can encapsulate your existing APIs into a collection that can be used by an AI. This allows you to give your AI the ability to perform actions that it wouldnâ€™t be able to do otherwise.
æ’ä»¶æ˜¯Semantic Kernelçš„å…³é”®ç»„ä»¶ã€‚å¦‚æœæ‚¨å·²ç»åœ¨ Microsoft 365 ä¸­ä½¿ç”¨è¿‡ ChatGPT æˆ– Copilot æ‰©å±•ä¸­çš„æ’ä»¶ï¼Œé‚£ä¹ˆæ‚¨å·²ç»ç†Ÿæ‚‰å®ƒä»¬ã€‚ä½¿ç”¨æ’ä»¶ï¼Œæ‚¨å¯ä»¥å°†ç°æœ‰ API å°è£…åˆ°å¯ä¾› AI ä½¿ç”¨çš„é›†åˆä¸­ã€‚è¿™ä½¿æ‚¨å¯ä»¥è®©æ‚¨çš„ AI èƒ½å¤Ÿæ‰§è¡Œå…¶ä»–æ–¹å¼æ— æ³•æ‰§è¡Œçš„ä½œã€‚

Behind the scenes, Semantic Kernel leverages [function calling](https://platform.openai.com/docs/guides/function-calling), a native feature of most of the latest LLMs to allow LLMs, to perform [planning](https://learn.microsoft.com/en-us/semantic-kernel/concepts/planning) and to invoke your APIs. With function calling, LLMs can request (i.e., call) a particular function. Semantic Kernel then marshals the request to the appropriate function in your codebase and returns the results back to the LLM so the LLM can generate a final response.
åœ¨å¹•åï¼ŒSemantic Kernelåˆ©ç”¨[å‡½æ•°è°ƒç”¨ ](https://platform.openai.com/docs/guides/function-calling)ï¼ˆå¤§å¤šæ•°æœ€æ–° LLM çš„åŸç”ŸåŠŸèƒ½ï¼‰æ¥å…è®¸ LLM æ‰§è¡Œ[è§„åˆ’](https://learn.microsoft.com/en-us/semantic-kernel/concepts/planning)å’Œè°ƒç”¨ APIã€‚é€šè¿‡å‡½æ•°è°ƒç”¨ï¼ŒLLM å¯ä»¥è¯·æ±‚ï¼ˆå³è°ƒç”¨ï¼‰ç‰¹å®šå‡½æ•°ã€‚ç„¶åï¼ŒSemantic Kernelå°†è¯·æ±‚ç¼–ç»„åˆ°ä»£ç åº“ä¸­çš„ç›¸åº”å‡½æ•°ï¼Œå¹¶å°†ç»“æœè¿”å›ç»™ LLMï¼Œä»¥ä¾¿ LLM å¯ä»¥ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚

![Semantic Kernel Plugin](https://learn.microsoft.com/en-us/semantic-kernel/media/designed-for-modular-extensibility-vertical.png)

Not all AI SDKs have an analogous concept to plugins (most just have functions or tools). In enterprise scenarios, however, plugins are valuable because they encapsulate a set of functionality that mirrors how enterprise developers already develop services and APIs. Plugins also play nicely with dependency injection. Within a plugin's constructor, you can inject services that are necessary to perform the work of the plugin (e.g., database connections, HTTP clients, etc.). This is difficult to accomplish with other SDKs that lack plugins.
å¹¶éæ‰€æœ‰ AI SDK éƒ½æœ‰ä¸æ’ä»¶ç±»ä¼¼çš„æ¦‚å¿µï¼ˆå¤§å¤šæ•°åªæ˜¯å…·æœ‰å‡½æ•°æˆ–å·¥å…·ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨ä¼ä¸šåœºæ™¯ä¸­ï¼Œæ’ä»¶å¾ˆæœ‰ä»·å€¼ï¼Œå› ä¸ºå®ƒä»¬å°è£…äº†ä¸€ç»„åŠŸèƒ½ï¼Œåæ˜ äº†ä¼ä¸šå¼€å‘äººå‘˜å·²ç»å¼€å‘æœåŠ¡å’Œ API çš„æ–¹å¼ã€‚æ’ä»¶ä¹Ÿå¯ä»¥å¾ˆå¥½åœ°ä¸ä¾èµ–æ³¨å…¥é…åˆä½¿ç”¨ã€‚åœ¨æ’ä»¶çš„æ„é€ å‡½æ•°ä¸­ï¼Œæ‚¨å¯ä»¥æ³¨å…¥æ‰§è¡Œæ’ä»¶å·¥ä½œæ‰€éœ€çš„æœåŠ¡ï¼ˆä¾‹å¦‚ï¼Œæ•°æ®åº“è¿æ¥ã€HTTP å®¢æˆ·ç«¯ç­‰ï¼‰ã€‚è¿™å¯¹äºå…¶ä»–ç¼ºå°‘æ’ä»¶çš„ SDK æ¥è¯´æ˜¯å¾ˆéš¾å®ç°çš„ã€‚



## Anatomy of a plugin  æ’ä»¶å‰–æ

At a high-level, a plugin is a group of [functions](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#importing-different-types-of-plugins) that can be exposed to AI apps and services. The functions within plugins can then be orchestrated by an AI application to accomplish user requests. Within Semantic Kernel, you can invoke these functions automatically with function calling.
ä»é«˜å±‚æ¬¡ä¸Šè®²ï¼Œæ’ä»¶æ˜¯ä¸€ç»„å¯ä»¥å‘ AI åº”ç”¨ç¨‹åºå’ŒæœåŠ¡[å…¬å¼€çš„åŠŸèƒ½ã€‚](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#importing-different-types-of-plugins) ç„¶åï¼Œæ’ä»¶ä¸­çš„å‡½æ•°å¯ä»¥ç”± AI åº”ç”¨ç¨‹åºç¼–æ’ä»¥å®Œæˆç”¨æˆ·è¯·æ±‚ã€‚åœ¨Semantic Kernelä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡å‡½æ•°è°ƒç”¨è‡ªåŠ¨è°ƒç”¨è¿™äº›å‡½æ•°ã€‚

 Note  æ³¨æ„

In other platforms, functions are often referred to as "tools" or "actions". In Semantic Kernel, we use the term "functions" since they are typically defined as native functions in your codebase.
åœ¨å…¶ä»–å¹³å°ä¸­ï¼Œå‡½æ•°é€šå¸¸è¢«ç§°ä¸ºâ€œå·¥å…·â€æˆ–â€œä½œâ€ã€‚åœ¨Semantic Kernelä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æœ¯è¯­â€œå‡½æ•°â€ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸åœ¨ä»£ç åº“ä¸­å®šä¹‰ä¸ºæœ¬æœºå‡½æ•°ã€‚

Just providing functions, however, is not enough to make a plugin. To power automatic orchestration with function calling, plugins also need to provide details that semantically describe how they behave. Everything from the function's input, output, and side effects need to be described in a way that the AI can understand, otherwise, the AI will not correctly call the function.
ç„¶è€Œï¼Œä»…ä»…æä¾›åŠŸèƒ½ä¸è¶³ä»¥åˆ¶ä½œæ’ä»¶ã€‚ä¸ºäº†é€šè¿‡å‡½æ•°è°ƒç”¨ä¸ºè‡ªåŠ¨ç¼–æ’æä¾›æ”¯æŒï¼Œæ’ä»¶è¿˜éœ€è¦æä¾›è¯­ä¹‰ä¸Šæè¿°å…¶è¡Œä¸ºæ–¹å¼çš„è¯¦ç»†ä¿¡æ¯ã€‚ä»å‡½æ•°çš„è¾“å…¥ã€è¾“å‡ºåˆ°å‰¯ä½œç”¨ï¼Œä¸€åˆ‡éƒ½éœ€è¦ä»¥ AI å¯ä»¥ç†è§£çš„æ–¹å¼è¿›è¡Œæè¿°ï¼Œå¦åˆ™ï¼ŒAI å°†æ— æ³•æ­£ç¡®è°ƒç”¨å‡½æ•°ã€‚

For example, the sample `WriterPlugin` plugin on the right has functions with semantic descriptions that describe what each function does. An LLM can then use these descriptions to choose the best functions to call to fulfill a user's ask.
ä¾‹å¦‚ï¼Œå³ä¾§çš„ç¤ºä¾‹ `WriterPlugin` æ’ä»¶å…·æœ‰è¯­ä¹‰æè¿°çš„å‡½æ•°ï¼Œè¿™äº›æè¿°æè¿°æ¯ä¸ªå‡½æ•°çš„ä½œç”¨ã€‚ç„¶åï¼ŒLLM å¯ä»¥ä½¿ç”¨è¿™äº›æè¿°æ¥é€‰æ‹©è¦è°ƒç”¨çš„æœ€ä½³å‡½æ•°æ¥æ»¡è¶³ç”¨æˆ·çš„è¦æ±‚ã€‚

In the picture on the right, an LLM would likely call the `ShortPoem` and `StoryGen` functions to satisfy the users ask thanks to the provided semantic descriptions.
åœ¨å³å›¾ä¸­ï¼ŒLLM å¯èƒ½ä¼šè°ƒç”¨ `ShortPoem` å’Œ `StoryGen` å‡½æ•°æ¥æ»¡è¶³ç”¨æˆ·å¯¹æä¾›çš„è¯­ä¹‰æè¿°çš„è¦æ±‚ã€‚

![Semantic description within the WriterPlugin plugin](https://learn.microsoft.com/en-us/semantic-kernel/media/writer-plugin-example.png)



### Importing different types of plugins å¯¼å…¥ä¸åŒç±»å‹çš„æ’ä»¶

There are three primary ways of importing plugins into Semantic Kernel: using [native code](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins), using an [OpenAPI specification](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins) or from a [MCP Server](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-mcp-plugins) The former allows you to author plugins in your existing codebase that can leverage dependencies and services you already have. The latter two allow you to import plugins from an OpenAPI specification or a MCP Server, which can be shared across different programming languages and platforms.
å°†æ’ä»¶å¯¼å…¥Semantic Kernelæœ‰ä¸‰ç§ä¸»è¦æ–¹æ³•ï¼šä½¿ç”¨[æœ¬æœºä»£ç  ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins)ã€ä½¿ç”¨ [OpenAPI è§„èŒƒ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins)æˆ–ä» [MCP æœåŠ¡å™¨](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-mcp-plugins)å‰è€…å…è®¸æ‚¨åœ¨ç°æœ‰ä»£ç åº“ä¸­åˆ›ä½œæ’ä»¶ï¼Œè¿™äº›æ’ä»¶å¯ä»¥åˆ©ç”¨æ‚¨å·²æœ‰çš„ä¾èµ–é¡¹å’ŒæœåŠ¡ã€‚åä¸¤è€…å…è®¸æ‚¨ä» OpenAPI è§„èŒƒæˆ– MCP æœåŠ¡å™¨å¯¼å…¥æ’ä»¶ï¼Œè¿™äº›æ’ä»¶å¯ä»¥åœ¨ä¸åŒçš„ç¼–ç¨‹è¯­è¨€å’Œå¹³å°ä¹‹é—´å…±äº«ã€‚

Below we provide a simple example of importing and using a native plugin. To learn more about how to import these different types of plugins, refer to the following articles:
ä¸‹é¢æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯¼å…¥å’Œä½¿ç”¨æœ¬æœºæ’ä»¶çš„ç®€å•ç¤ºä¾‹ã€‚è¦äº†è§£æœ‰å…³å¦‚ä½•å¯¼å…¥è¿™äº›ä¸åŒç±»å‹çš„æ’ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹æ–‡ç« ï¼š

- [Importing native code  å¯¼å…¥æœ¬æœºä»£ç ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins)
- [Importing an OpenAPI specification
  å¯¼å…¥ OpenAPI è§„èŒƒ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins)
- [Importing a MCP Server
  å¯¼å…¥ MCP æœåŠ¡å™¨](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-mcp-plugins)

 Tip  æç¤º

When getting started, we recommend using native code plugins. As your application matures, and as you work across cross-platform teams, you may want to consider using OpenAPI specifications to share plugins across different programming languages and platforms. You can then also create a MCP Server from your Kernel instance, which allows other applications to consume your plugins as a service.
å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æœ¬æœºä»£ç æ’ä»¶ã€‚éšç€åº”ç”¨ç¨‹åºçš„æˆç†Ÿï¼Œä»¥åŠè·¨å¹³å°å›¢é˜Ÿçš„å·¥ä½œï¼Œæ‚¨å¯èƒ½éœ€è¦è€ƒè™‘ä½¿ç”¨ OpenAPI è§„èŒƒåœ¨ä¸åŒçš„ç¼–ç¨‹è¯­è¨€å’Œå¹³å°ä¸Šå…±äº«æ’ä»¶ã€‚ç„¶åï¼Œæ‚¨è¿˜å¯ä»¥ä»å†…æ ¸å®ä¾‹åˆ›å»º MCP æœåŠ¡å™¨ï¼Œè¿™å…è®¸å…¶ä»–åº”ç”¨ç¨‹åºå°†æ‚¨çš„æ’ä»¶ä½œä¸ºæœåŠ¡ä½¿ç”¨ã€‚



### The different types of plugin functions ä¸åŒç±»å‹çš„æ’ä»¶åŠŸèƒ½

Within a plugin, you will typically have two different types of functions, those that retrieve data for retrieval augmented generation (RAG) and those that automate tasks. While each type is functionally the same, they are typically used differently within applications that use Semantic Kernel.
åœ¨æ’ä»¶ä¸­ï¼Œæ‚¨é€šå¸¸ä¼šæœ‰ä¸¤ç§ä¸åŒç±»å‹çš„å‡½æ•°ï¼Œä¸€ç§æ˜¯æ£€ç´¢æ•°æ®ä»¥è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆ ï¼ˆRAGï¼‰ çš„å‡½æ•°ï¼Œå¦ä¸€ç§æ˜¯è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡çš„å‡½æ•°ã€‚è™½ç„¶æ¯ç§ç±»å‹åœ¨åŠŸèƒ½ä¸Šéƒ½ç›¸åŒï¼Œä½†å®ƒä»¬åœ¨ä½¿ç”¨Semantic Kernelçš„åº”ç”¨ç¨‹åºä¸­çš„ä½¿ç”¨æ–¹å¼é€šå¸¸ä¸åŒã€‚

For example, with retrieval functions, you may want to use strategies to improve performance (e.g., caching and using cheaper intermediate models for summarization). Whereas with task automation functions, you'll likely want to implement human-in-the-loop approval processes to ensure that tasks are completed correctly.
ä¾‹å¦‚ï¼Œå¯¹äºæ£€ç´¢å‡½æ•°ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨ç­–ç•¥æ¥æé«˜æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œç¼“å­˜å’Œä½¿ç”¨æ›´ä¾¿å®œçš„ä¸­é—´æ¨¡å‹è¿›è¡Œæ±‡æ€»ï¼‰ã€‚è€Œå¯¹äºä»»åŠ¡è‡ªåŠ¨åŒ–åŠŸèƒ½ï¼Œæ‚¨å¯èƒ½éœ€è¦å®æ–½äººæœºäº¤äº’å®¡æ‰¹æµç¨‹ï¼Œä»¥ç¡®ä¿ä»»åŠ¡æ­£ç¡®å®Œæˆã€‚

To learn more about the different types of plugin functions, refer to the following articles:
è¦äº†è§£æœ‰å…³ä¸åŒç±»å‹çš„æ’ä»¶åŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹æ–‡ç« ï¼š

- [Data retrieval functions
  æ•°æ®æ£€ç´¢åŠŸèƒ½](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/using-data-retrieval-functions-for-rag)
- [Task automation functions
  ä»»åŠ¡è‡ªåŠ¨åŒ–åŠŸèƒ½](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/using-task-automation-functions)



## Getting started with plugins æ’ä»¶å…¥é—¨

Using plugins within Semantic Kernel is always a three step process:
åœ¨Semantic Kernelä¸­ä½¿ç”¨æ’ä»¶å§‹ç»ˆæ˜¯ä¸€ä¸ªä¸‰æ­¥è¿‡ç¨‹ï¼š

1. [Define your plugin  å®šä¹‰æ’ä»¶](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#1-define-your-plugin)
2. [Add the plugin to your kernel
   å°†æ’ä»¶æ·»åŠ åˆ°å†…æ ¸](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#2-add-the-plugin-to-your-kernel)
3. [And then either invoke the plugin's functions in either a prompt with function calling
   ç„¶ååœ¨å¸¦æœ‰å‡½æ•°è°ƒç”¨çš„æç¤ºä¸­è°ƒç”¨æ’ä»¶çš„å‡½æ•°](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-csharp#3-invoke-the-plugins-functions)

Below we'll provide a high-level example of how to use a plugin within Semantic Kernel. Refer to the links above for more detailed information on how to create and use plugins.
ä¸‹é¢æˆ‘ä»¬å°†æä¾›å¦‚ä½•åœ¨Semantic Kernelä¸­ä½¿ç”¨æ’ä»¶çš„é«˜çº§ç¤ºä¾‹ã€‚æœ‰å…³å¦‚ä½•åˆ›å»ºå’Œä½¿ç”¨æ’ä»¶çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šé¢çš„é“¾æ¥ã€‚



### 1) Define your plugin  1ï¼‰ å®šä¹‰ä½ çš„æ’ä»¶

The easiest way to create a plugin is by defining a class and annotating its methods with the `KernelFunction` attribute. This let's Semantic Kernel know that this is a function that can be called by an AI or referenced in a prompt.
åˆ›å»ºæ’ä»¶çš„æœ€ç®€å•æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªç±»å¹¶ä½¿ç”¨ `KernelFunction` å±æ€§æ³¨é‡Šå…¶æ–¹æ³•ã€‚è¿™è®©Semantic KernelçŸ¥é“è¿™æ˜¯ä¸€ä¸ªå¯ä»¥ç”± AI è°ƒç”¨æˆ–åœ¨æç¤ºä¸­å¼•ç”¨çš„å‡½æ•°ã€‚

You can also import plugins from an [OpenAPI specification](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins).
æ‚¨è¿˜å¯ä»¥ä» [OpenAPI è§„èŒƒ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins)å¯¼å…¥æ’ä»¶ã€‚

Below, we'll create a plugin that can retrieve the state of lights and alter its state.
ä¸‹é¢ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ’ä»¶ï¼Œå¯ä»¥æ£€ç´¢å…‰æºçš„çŠ¶æ€å¹¶æ›´æ”¹å…¶çŠ¶æ€ã€‚

 Tip  æç¤º

Since most LLM have been trained with Python for function calling, its recommended to use snake case for function names and property names even if you're using the C# or Java SDK.
ç”±äºå¤§å¤šæ•° LLM éƒ½å·²ä½¿ç”¨ Python è¿›è¡Œå‡½æ•°è°ƒç”¨è®­ç»ƒï¼Œå› æ­¤å³ä½¿æ‚¨ä½¿ç”¨çš„æ˜¯ C# æˆ– Java SDKï¼Œä¹Ÿå»ºè®®ä½¿ç”¨è›‡å½¢å‘½åæ³•ä½œä¸ºå‡½æ•°åç§°å’Œå±æ€§åç§°ã€‚

C#Copy  å¤åˆ¶

```csharp
using System.ComponentModel;
using Microsoft.SemanticKernel;

public class LightsPlugin
{
   // Mock data for the lights
   private readonly List<LightModel> lights = new()
   {
      new LightModel { Id = 1, Name = "Table Lamp", IsOn = false, Brightness = 100, Hex = "FF0000" },
      new LightModel { Id = 2, Name = "Porch light", IsOn = false, Brightness = 50, Hex = "00FF00" },
      new LightModel { Id = 3, Name = "Chandelier", IsOn = true, Brightness = 75, Hex = "0000FF" }
   };

   [KernelFunction("get_lights")]
   [Description("Gets a list of lights and their current state")]
   public async Task<List<LightModel>> GetLightsAsync()
   {
      return lights
   }

   [KernelFunction("get_state")]
   [Description("Gets the state of a particular light")]
   public async Task<LightModel?> GetStateAsync([Description("The ID of the light")] int id)
   {
      // Get the state of the light with the specified ID
      return lights.FirstOrDefault(light => light.Id == id);
   }

   [KernelFunction("change_state")]
   [Description("Changes the state of the light")]
   public async Task<LightModel?> ChangeStateAsync(int id, LightModel LightModel)
   {
      var light = lights.FirstOrDefault(light => light.Id == id);

      if (light == null)
      {
         return null;
      }

      // Update the light with the new state
      light.IsOn = LightModel.IsOn;
      light.Brightness = LightModel.Brightness;
      light.Hex = LightModel.Hex;

      return light;
   }
}

public class LightModel
{
   [JsonPropertyName("id")]
   public int Id { get; set; }

   [JsonPropertyName("name")]
   public string Name { get; set; }

   [JsonPropertyName("is_on")]
   public bool? IsOn { get; set; }

   [JsonPropertyName("brightness")]
   public byte? Brightness { get; set; }

   [JsonPropertyName("hex")]
   public string? Hex { get; set; }
}
```

Notice that we provide descriptions for the function, and parameters. This is important for the AI to understand what the function does and how to use it.
è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æä¾›äº†å‡½æ•°å’Œå‚æ•°çš„æè¿°ã€‚è¿™å¯¹äºäººå·¥æ™ºèƒ½äº†è§£è¯¥åŠŸèƒ½çš„ä½œç”¨ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒéå¸¸é‡è¦ã€‚

 Tip  æç¤º

Don't be afraid to provide detailed descriptions for your functions if an AI is having trouble calling them. Few-shot examples, recommendations for when to use (and not use) the function, and guidance on where to get required parameters can all be helpful.
å¦‚æœ AI åœ¨è°ƒç”¨å‡½æ•°æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·ä¸è¦å®³æ€•ä¸ºæ‚¨çš„å‡½æ•°æä¾›è¯¦ç»†æè¿°ã€‚å°‘é‡ç¤ºä¾‹ã€ä½•æ—¶ä½¿ç”¨ï¼ˆå’Œä¸ä½¿ç”¨ï¼‰å‡½æ•°çš„å»ºè®®ä»¥åŠæœ‰å…³ä»ä½•å¤„è·å–æ‰€éœ€å‚æ•°çš„æŒ‡å—éƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚



### 2) Add the plugin to your kernel 2ï¼‰ å°†æ’ä»¶æ·»åŠ åˆ°æ‚¨çš„å†…æ ¸ä¸­

Once you've defined your plugin, you can add it to your kernel by creating a new instance of the plugin and adding it to the kernel's plugin collection.
å®šä¹‰æ’ä»¶åï¼Œæ‚¨å¯ä»¥é€šè¿‡åˆ›å»ºæ’ä»¶çš„æ–°å®ä¾‹å¹¶å°†å…¶æ·»åŠ åˆ°å†…æ ¸çš„æ’ä»¶é›†åˆä¸­æ¥å°†å…¶æ·»åŠ åˆ°å†…æ ¸ä¸­ã€‚

This example demonstrates the easiest way of adding a class as a plugin with the `AddFromType` method. To learn about other ways of adding plugins, refer to the [adding native plugins](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins) article.
æ­¤ç¤ºä¾‹æ¼”ç¤ºäº†ä½¿ç”¨ `AddFromType` æ–¹æ³•å°†ç±»æ·»åŠ ä¸ºæ’ä»¶çš„æœ€ç®€å•æ–¹æ³•ã€‚è¦äº†è§£æ·»åŠ æ’ä»¶çš„å…¶ä»–æ–¹æ³•ï¼Œè¯·å‚é˜…[æ·»åŠ æœ¬æœºæ’ä»¶](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins)ä¸€æ–‡ã€‚

C#Copy  å¤åˆ¶

```csharp
var builder = new KernelBuilder();
builder.Plugins.AddFromType<LightsPlugin>("Lights")
Kernel kernel = builder.Build();
```



### 3) Invoke the plugin's functions 3ï¼‰è°ƒç”¨æ’ä»¶çš„åŠŸèƒ½

Finally, you can have the AI invoke your plugin's functions by using function calling. Below is an example that demonstrates how to coax the AI to call the `get_lights` function from the `Lights` plugin before calling the `change_state` function to turn on a light.
æœ€åï¼Œä½ å¯ä»¥è®© AI ä½¿ç”¨å‡½æ•°è°ƒç”¨æ¥è°ƒç”¨æ’ä»¶çš„å‡½æ•°ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ¼”ç¤ºäº†å¦‚ä½•åœ¨è°ƒç”¨ `change_state` å‡½æ•°æ‰“å¼€ç¯æºä¹‹å‰ï¼Œè¯±ä½¿ AI ä» `Lights` æ’ä»¶è°ƒç”¨ `get_lights` å‡½æ•°ã€‚

C#Copy  å¤åˆ¶

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.OpenAI;

// Create a kernel with Azure OpenAI chat completion
var builder = Kernel.CreateBuilder().AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);

// Build the kernel
Kernel kernel = builder.Build();
var chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();

// Add a plugin (the LightsPlugin class is defined below)
kernel.Plugins.AddFromType<LightsPlugin>("Lights");

// Enable planning
OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new() 
{
    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()
};

// Create a history store the conversation
var history = new ChatHistory();
history.AddUserMessage("Please turn on the lamp");

// Get the response from the AI
var result = await chatCompletionService.GetChatMessageContentAsync(
   history,
   executionSettings: openAIPromptExecutionSettings,
   kernel: kernel);

// Print the results
Console.WriteLine("Assistant > " + result);

// Add the message from the agent to the chat history
history.AddAssistantMessage(result);
```

With the above code, you should get a response that looks like the following:
ä½¿ç”¨ä¸Šé¢çš„ä»£ç ï¼Œæ‚¨åº”è¯¥ä¼šå¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„å“åº”ï¼š

  å±•å¼€è¡¨

| Role  è§’è‰²                                               | Message  æ¶ˆæ¯                                                |
| :------------------------------------------------------- | :----------------------------------------------------------- |
| ğŸ”µ **User**  ğŸ”µ **ç”¨æˆ·**                                   | Please turn on the lamp è¯·æ‰“å¼€ lamp                          |
| ğŸ”´ **Assistant (function call)** ğŸ”´ **åŠ©æ‰‹ ï¼ˆå‡½æ•° è°ƒç”¨ï¼‰** | `Lights.get_lights()`                                        |
| ğŸŸ¢ **Tool**  ğŸŸ¢ **å·¥å…·**                                   | `[{ "id": 1, "name": "Table Lamp", "isOn": false, "brightness": 100, "hex": "FF0000" }, { "id": 2, "name": "Porch light", "isOn": false, "brightness": 50, "hex": "00FF00" }, { "id": 3, "name": "Chandelier", "isOn": true, "brightness": 75, "hex": "0000FF" }]` |
| ğŸ”´ **Assistant (function call)** ğŸ”´ **åŠ©æ‰‹ ï¼ˆå‡½æ•° è°ƒç”¨ï¼‰** | Lights.change_state(1, { "isOn": true }) Lights.change_stateï¼ˆ1ï¼Œ { â€œisOnâ€ï¼š true }ï¼‰ |
| ğŸŸ¢ **Tool**  ğŸŸ¢ **å·¥å…·**                                   | `{ "id": 1, "name": "Table Lamp", "isOn": true, "brightness": 100, "hex": "FF0000" }` |
| ğŸ”´ **Assistant**  ğŸ”´ **åŠ©ç†**                              | The lamp is now on ç¯ç°åœ¨äº®äº†                                |

 Tip  æç¤º

While you can invoke a plugin function directly, this is not advised because the AI should be the one deciding which functions to call. If you need explicit control over which functions are called, consider using standard methods in your codebase instead of plugins.
è™½ç„¶ä½ å¯ä»¥ç›´æ¥è°ƒç”¨æ’ä»¶å‡½æ•°ï¼Œä½†ä¸å»ºè®®è¿™æ ·åšï¼Œå› ä¸º AI åº”è¯¥å†³å®šè¦è°ƒç”¨å“ªäº›å‡½æ•°ã€‚å¦‚æœæ‚¨éœ€è¦æ˜¾å¼æ§åˆ¶è°ƒç”¨å“ªäº›å‡½æ•°ï¼Œè¯·è€ƒè™‘åœ¨ä»£ç åº“ä¸­ä½¿ç”¨æ ‡å‡†æ–¹æ³•è€Œä¸æ˜¯æ’ä»¶ã€‚



## General recommendations for authoring plugins åˆ›ä½œæ’ä»¶çš„ä¸€èˆ¬å»ºè®®

Considering that each scenario has unique requirements, utilizes distinct plugin designs, and may incorporate multiple LLMs, it is challenging to provide a one-size-fits-all guide for plugin design. However, below are some general recommendations and guidelines to ensure that plugins are AI-friendly and can be easily and efficiently consumed by LLMs.
è€ƒè™‘åˆ°æ¯ä¸ªåœºæ™¯éƒ½æœ‰ç‹¬ç‰¹çš„è¦æ±‚ï¼Œä½¿ç”¨ä¸åŒçš„æ’ä»¶è®¾è®¡ï¼Œå¹¶ä¸”å¯èƒ½åŒ…å«å¤šä¸ª LLMï¼Œå› æ­¤ä¸ºæ’ä»¶è®¾è®¡æä¾›ä¸€åˆ€åˆ‡çš„æŒ‡å—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä½†æ˜¯ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›ä¸€èˆ¬å»ºè®®å’ŒæŒ‡å—ï¼Œä»¥ç¡®ä¿æ’ä»¶å¯¹ AI å‹å¥½å¹¶ä¸” LLM å¯ä»¥è½»æ¾é«˜æ•ˆåœ°ä½¿ç”¨ã€‚



### Import only the necessary plugins ä»…å¯¼å…¥å¿…è¦çš„æ’ä»¶

Import only the plugins that contain functions necessary for your specific scenario. This approach will not only reduce the number of input tokens consumed but also minimize the occurrence of function miscalls-calls to functions that are not used in the scenario. Overall, this strategy should enhance function-calling accuracy and decrease the number of false positives.
ä»…å¯¼å…¥åŒ…å«ç‰¹å®šåœºæ™¯æ‰€éœ€åŠŸèƒ½çš„æ’ä»¶ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘æ¶ˆè€—çš„è¾“å…¥ä»¤ç‰Œæ•°ï¼Œè¿˜å¯ä»¥æœ€å¤§é™åº¦åœ°å‡å°‘å‡½æ•°é”™è¯¯è°ƒç”¨çš„å‘ç”Ÿ - å¯¹æ–¹æ¡ˆä¸­æœªä½¿ç”¨çš„å‡½æ•°çš„è°ƒç”¨ã€‚æ€»ä½“è€Œè¨€ï¼Œæ­¤ç­–ç•¥åº”æé«˜å‡½æ•°è°ƒç”¨å‡†ç¡®æ€§å¹¶å‡å°‘è¯¯æŠ¥æ•°é‡ã€‚

Additionally, OpenAI recommends that you use no more than 20 tools in a single API call; ideally, no more than 10 tools. As stated by OpenAI: *"We recommend that you use no more than 20 tools in a single API call. Developers typically see a reduction in the model's ability to select the correct tool once they have between 10-20 tools defined."** For more information, you can visit their documentation at [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling#keep-the-number-of-functions-low-for-higher-accuracy).
æ­¤å¤–ï¼ŒOpenAI å»ºè®®æ‚¨åœ¨å•ä¸ª API è°ƒç”¨ä¸­ä½¿ç”¨ä¸è¶…è¿‡ 20 ä¸ªå·¥å…·;ç†æƒ³æƒ…å†µä¸‹ï¼Œä¸è¶…è¿‡ 10 ä¸ªå·¥å…·ã€‚æ­£å¦‚ OpenAI æ‰€è¯´ï¼šâ€œ *æˆ‘ä»¬å»ºè®®æ‚¨åœ¨å•ä¸ª API è°ƒç”¨ä¸­ä½¿ç”¨ä¸è¶…è¿‡ 20 ä¸ªå·¥å…·ã€‚ä¸€æ—¦å¼€å‘äººå‘˜å®šä¹‰äº† 10-20 ä¸ªå·¥å…·ï¼Œä»–ä»¬é€šå¸¸ä¼šçœ‹åˆ°æ¨¡å‹é€‰æ‹©æ­£ç¡®å·¥å…·çš„èƒ½åŠ›ä¸‹é™ã€‚** æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è®¿é—®ä»–ä»¬çš„æ–‡æ¡£ï¼Œç½‘å€ä¸º [OpenAI å‡½æ•°è°ƒç”¨æŒ‡å— ](https://platform.openai.com/docs/guides/function-calling#keep-the-number-of-functions-low-for-higher-accuracy)ã€‚



### Make plugins AI-friendly  ä½¿æ’ä»¶å¯¹äººå·¥æ™ºèƒ½å‹å¥½

To enhance the LLM's ability to understand and utilize plugins, it is recommended to follow these guidelines:
ä¸ºäº†å¢å¼º LLM ç†è§£å’Œä½¿ç”¨æ’ä»¶çš„èƒ½åŠ›ï¼Œå»ºè®®éµå¾ªä»¥ä¸‹å‡†åˆ™ï¼š

- **Use descriptive and concise function names:** Ensure that function names clearly convey their purpose to help the model understand when to select each function. If a function name is ambiguous, consider renaming it for clarity. Avoid using abbreviations or acronyms to shorten function names. Utilize the `DescriptionAttribute` to provide additional context and instructions only when necessary, minimizing token consumption.
  **ä½¿ç”¨æè¿°æ€§å’Œç®€æ´çš„å‡½æ•°åç§°ï¼š** ç¡®ä¿å‡½æ•°åç§°æ¸…æ¥šåœ°ä¼ è¾¾å…¶ç”¨é€”ï¼Œä»¥å¸®åŠ©æ¨¡å‹äº†è§£ä½•æ—¶é€‰æ‹©æ¯ä¸ªå‡½æ•°ã€‚å¦‚æœå‡½æ•°åç§°ä¸æ˜ç¡®ï¼Œè¯·è€ƒè™‘é‡å‘½åä»¥æ¸…æ¥šèµ·è§ã€‚é¿å…ä½¿ç”¨ç¼©å†™æˆ–é¦–å­—æ¯ç¼©ç•¥è¯æ¥ç¼©çŸ­å‡½æ•°åç§°ã€‚ä»…åœ¨å¿…è¦æ—¶åˆ©ç”¨ `DescriptionAttribute` æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡å’Œè¯´æ˜ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘ä»¤ç‰Œæ¶ˆè€—ã€‚
- **Minimize function parameters:** Limit the number of function parameters and use primitive types whenever possible. This approach reduces token consumption and simplifies the function signature, making it easier for the LLM to match function parameters effectively.
  **æœ€å°åŒ–å‡½æ•°å‚æ•°ï¼š** é™åˆ¶å‡½æ•°å‚æ•°çš„æ•°é‡ï¼Œå¹¶å°½å¯èƒ½ä½¿ç”¨åŸºå…ƒç±»å‹ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†ä»¤ç‰Œæ¶ˆè€—å¹¶ç®€åŒ–äº†å‡½æ•°ç­¾åï¼Œä½¿ LLM æ›´å®¹æ˜“æœ‰æ•ˆåœ°åŒ¹é…å‡½æ•°å‚æ•°ã€‚
- **Name function parameters clearly:** Assign descriptive names to function parameters to clarify their purpose. Avoid using abbreviations or acronyms to shorten parameter names, as this will assist the LLM in reasoning about the parameters and providing accurate values. As with function names, use the `DescriptionAttribute` only when necessary to minimize token consumption.
  **æ˜ç¡®å‘½åå‡½æ•°å‚æ•°ï¼š** ä¸ºå‡½æ•°å‚æ•°åˆ†é…æè¿°æ€§åç§°ä»¥é˜æ˜å…¶ç”¨é€”ã€‚é¿å…ä½¿ç”¨ç¼©å†™æˆ–é¦–å­—æ¯ç¼©ç•¥è¯æ¥ç¼©çŸ­å‚æ•°åç§°ï¼Œå› ä¸ºè¿™å°†æœ‰åŠ©äº LLM æ¨ç†å‚æ•°å¹¶æä¾›å‡†ç¡®çš„å€¼ã€‚ä¸å‡½æ•°åç§°ä¸€æ ·ï¼Œä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨ `DescriptionAttribute` ä»¥æœ€å¤§ç¨‹åº¦åœ°å‡å°‘ä»¤ç‰Œæ¶ˆè€—ã€‚



### Find a right balance between the number of functions and their responsibilities åœ¨èŒèƒ½æ•°é‡åŠå…¶èŒè´£ä¹‹é—´æ‰¾åˆ°é€‚å½“çš„å¹³è¡¡

On one hand, having functions with a single responsibility is a good practice that allows to keep functions simple and reusable across multiple scenarios. On the other hand, each function call incurs overhead in terms of network round-trip latency and the number of consumed input and output tokens: input tokens are used to send the function definition and invocation result to the LLM, while output tokens are consumed when receiving the function call from the model. Alternatively, a single function with multiple responsibilities can be implemented to reduce the number of consumed tokens and lower network overhead, although this comes at the cost of reduced reusability in other scenarios.
ä¸€æ–¹é¢ï¼Œå°†å‡½æ•°å…·æœ‰å•ä¸€èŒè´£æ˜¯ä¸€ç§å¾ˆå¥½çš„åšæ³•ï¼Œå®ƒå…è®¸ä½¿å‡½æ•°ä¿æŒç®€å•ä¸”å¯åœ¨å¤šä¸ªæ–¹æ¡ˆä¸­é‡ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œæ¯æ¬¡å‡½æ•°è°ƒç”¨åœ¨ç½‘ç»œå¾€è¿”å»¶è¿Ÿå’Œæ¶ˆè€—çš„è¾“å…¥å’Œè¾“å‡º token æ•°é‡æ–¹é¢éƒ½ä¼šäº§ç”Ÿå¼€é”€ï¼šè¾“å…¥ token ç”¨äºå°†å‡½æ•°å®šä¹‰å’Œè°ƒç”¨ç»“æœå‘é€ç»™ LLMï¼Œè€Œè¾“å‡º token åˆ™åœ¨æ¥æ”¶æ¨¡å‹çš„å‡½æ•°è°ƒç”¨æ—¶æ¶ˆè€—ã€‚æˆ–è€…ï¼Œå¯ä»¥å®ç°å…·æœ‰å¤šä¸ªèŒè´£çš„å•ä¸ªåŠŸèƒ½ï¼Œä»¥å‡å°‘æ¶ˆè€—çš„ä»£å¸æ•°é‡å¹¶é™ä½ç½‘ç»œå¼€é”€ï¼Œå°½ç®¡è¿™æ˜¯ä»¥é™ä½å…¶ä»–åœºæ™¯çš„å¯é‡ç”¨æ€§ä¸ºä»£ä»·çš„ã€‚

However, consolidating many responsibilities into a single function may increase the number and complexity of function parameters and its return type. This complexity can lead to situations where the model may struggle to correctly match the function parameters, resulting in missed parameters or values of incorrect type. Therefore, it is essential to strike the right balance between the number of functions to reduce network overhead and the number of responsibilities each function has, ensuring that the model can accurately match function parameters.
ä½†æ˜¯ï¼Œå°†è®¸å¤šèŒè´£åˆå¹¶åˆ°å•ä¸ªå‡½æ•°ä¸­å¯èƒ½ä¼šå¢åŠ å‡½æ•°å‚æ•°åŠå…¶è¿”å›ç±»å‹çš„æ•°é‡å’Œå¤æ‚æ€§ã€‚è¿™ç§å¤æ‚æ€§å¯èƒ½å¯¼è‡´æ¨¡å‹å¯èƒ½éš¾ä»¥æ­£ç¡®åŒ¹é…å‡½æ•°å‚æ•°ï¼Œä»è€Œå¯¼è‡´é—æ¼å‚æ•°æˆ–ç±»å‹ä¸æ­£ç¡®çš„å€¼ã€‚å› æ­¤ï¼Œå¿…é¡»åœ¨å‡å°‘ç½‘ç»œå¼€é”€çš„å‡½æ•°æ•°é‡å’Œæ¯ä¸ªå‡½æ•°çš„èŒè´£æ•°é‡ä¹‹é—´å–å¾—é€‚å½“çš„å¹³è¡¡ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åŒ¹é…å‡½æ•°å‚æ•°ã€‚



### Transform Semantic Kernel functions è½¬æ¢Semantic Kernelå‡½æ•°

Utilize the transformation techniques for Semantic Kernel functions as described in the [Transforming Semantic Kernel Functions](https://devblogs.microsoft.com/semantic-kernel/transforming-semantic-kernel-functions/#:~:text=Semantic Kernel provides a series,from an Open API specification) blog post to:
åˆ©ç”¨Semantic Kernelå‡½æ•°çš„è½¬æ¢æŠ€æœ¯ï¼Œå¦‚è½¬æ¢è¯­[ä¹‰å†…æ ¸å‡½æ•°](https://devblogs.microsoft.com/semantic-kernel/transforming-semantic-kernel-functions/#:~:text=Semantic Kernel provides a series,from an Open API specification)åšå®¢æ–‡ç« ä¸­æ‰€è¿°ï¼Œå¯ä»¥ï¼š

- **Change function behavior:** There are scenarios where the default behavior of a function may not align with the desired outcome and it's not feasible to modify the original function's implementation. In such cases, you can create a new function that wraps the original one and modifies its behavior accordingly.
  **æ›´æ”¹åŠŸèƒ½è¡Œä¸ºï¼š** åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå‡½æ•°çš„é»˜è®¤è¡Œä¸ºå¯èƒ½ä¸é¢„æœŸç»“æœä¸ä¸€è‡´ï¼Œå¹¶ä¸”ä¿®æ”¹åŸå§‹å‡½æ•°çš„å®ç°æ˜¯ä¸å¯è¡Œçš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°å‡½æ•°æ¥åŒ…è£…åŸå§‹å‡½æ•°å¹¶ç›¸åº”åœ°ä¿®æ”¹å…¶è¡Œä¸ºã€‚
- **Provide context information:** Functions may require parameters that the LLM cannot or should not infer. For example, if a function needs to act on behalf of the current user or requires authentication information, this context is typically available to the host application but not to the LLM. In such cases, you can transform the function to invoke the original one while supplying the necessary context information from the hosting application, along with arguments provided by the LLM.
  **æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š** å‡½æ•°å¯èƒ½éœ€è¦ LLM æ— æ³•æˆ–ä¸åº”è¯¥æ¨æ–­çš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‡½æ•°éœ€è¦ä»£è¡¨å½“å‰ç”¨æˆ·æ‰§è¡Œä½œæˆ–éœ€è¦èº«ä»½éªŒè¯ä¿¡æ¯ï¼Œåˆ™æ­¤ä¸Šä¸‹æ–‡é€šå¸¸å¯ä¾›ä¸»æœºåº”ç”¨ç¨‹åºä½¿ç”¨ï¼Œä½†ä¸é€‚ç”¨äº LLMã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥è½¬æ¢å‡½æ•°ä»¥è°ƒç”¨åŸå§‹å‡½æ•°ï¼ŒåŒæ—¶ä»æ‰˜ç®¡åº”ç”¨ç¨‹åºæä¾›å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠ LLM æä¾›çš„å‚æ•°ã€‚
- **Change parameters list, types, and names:** If the original function has a complex signature that the LLM struggles to interpret, you can transform the function into one with a simpler signature that the LLM can more easily understand. This may involve changing parameter names, types, the number of parameters, and flattening or unflattening complex parameters, among other adjustments.
  **æ›´æ”¹å‚æ•°åˆ—è¡¨ã€ç±»å‹å’Œåç§°ï¼š** å¦‚æœåŸå§‹å‡½æ•°å…·æœ‰ LLM éš¾ä»¥è§£é‡Šçš„å¤æ‚ç­¾åï¼Œæ‚¨å¯ä»¥å°†è¯¥å‡½æ•°è½¬æ¢ä¸ºå…·æœ‰ LLM æ›´å®¹æ˜“ç†è§£çš„æ›´ç®€å•ç­¾åçš„å‡½æ•°ã€‚è¿™å¯èƒ½æ¶‰åŠæ›´æ”¹å‚æ•°åç§°ã€ç±»å‹ã€å‚æ•°æ•°é‡ä»¥åŠå±•å¹³æˆ–å–æ¶ˆå±•å¹³å¤æ‚å‚æ•°ç­‰è°ƒæ•´ã€‚



### Local state utilization  æœ¬åœ°çŠ¶æ€åˆ©ç”¨ç‡

When designing plugins that operate on relatively large or confidential datasets, such as documents, articles, or emails containing sensitive information, consider utilizing local state to store original data or intermediate results that do not need to be sent to the LLM. Functions for such scenarios can accept and return a state id, allowing you to look up and access the data locally instead of passing the actual data to the LLM, only to receive it back as an argument for the next function invocation.
åœ¨è®¾è®¡å¯¹ç›¸å¯¹è¾ƒå¤§æˆ–æœºå¯†æ•°æ®é›†ï¼ˆä¾‹å¦‚åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ–‡æ¡£ã€æ–‡ç« æˆ–ç”µå­é‚®ä»¶ï¼‰è¿›è¡Œä½œçš„æ’ä»¶æ—¶ï¼Œè¯·è€ƒè™‘åˆ©ç”¨æœ¬åœ°çŠ¶æ€æ¥å­˜å‚¨ä¸éœ€è¦å‘é€åˆ° LLM çš„åŸå§‹æ•°æ®æˆ–ä¸­é—´ç»“æœã€‚æ­¤ç±»åœºæ™¯çš„å‡½æ•°å¯ä»¥æ¥å—å¹¶è¿”å›çŠ¶æ€ IDï¼Œå…è®¸æ‚¨åœ¨æœ¬åœ°æŸ¥æ‰¾å’Œè®¿é—®æ•°æ®ï¼Œè€Œä¸æ˜¯å°†å®é™…æ•°æ®ä¼ é€’ç»™ LLMï¼Œåªæ˜¯å°†å…¶ä½œä¸ºä¸‹ä¸€ä¸ªå‡½æ•°è°ƒç”¨çš„å‚æ•°æ¥æ”¶å›æ¥ã€‚

By storing data locally, you can keep the information private and secure while avoiding unnecessary token consumption during function calls. This approach not only enhances data privacy but also improves overall efficiency in processing large or sensitive datasets.
é€šè¿‡åœ¨æœ¬åœ°å­˜å‚¨æ•°æ®ï¼Œå¯ä»¥ä¿æŒä¿¡æ¯çš„ç§å¯†æ€§å’Œå®‰å…¨æ€§ï¼ŒåŒæ—¶é¿å…åœ¨å‡½æ•°è°ƒç”¨æœŸé—´ä¸å¿…è¦çš„ä»¤ç‰Œæ¶ˆè€—ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¢å¼ºäº†æ•°æ®éšç§ï¼Œè¿˜æé«˜äº†å¤„ç†å¤§å‹æˆ–æ•æ„Ÿæ•°æ®é›†çš„æ•´ä½“æ•ˆç‡ã€‚



### Provide function return type schema to AI model å‘ AI æ¨¡å‹æä¾›å‡½æ•°è¿”å›ç±»å‹æ¶æ„

Use one of the techniques described in the [Providing functions return type schema to LLM](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins#provide-function-return-type-information-in-function-description) section to provide the function's return type schema to the AI model.
ä½¿ç”¨ å‘ [LLM æä¾›å‡½æ•°è¿”å›ç±»å‹æ¶æ„ ](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-native-plugins#provide-function-return-type-information-in-function-description)éƒ¨åˆ†ä¸­æ‰€è¿°çš„æŠ€æœ¯ä¹‹ä¸€ï¼Œå‘ AI æ¨¡å‹æä¾›å‡½æ•°çš„è¿”å›ç±»å‹æ¶æ„ã€‚

By utilizing a well-defined return type schema, the AI model can accurately identify the intended properties, eliminating potential inaccuracies that may arise when the model makes assumptions based on incomplete or ambiguous information in the absence of the schema. Consequently, this enhances the accuracy of function calls, leading to more reliable and precise outcomes.
é€šè¿‡åˆ©ç”¨å®šä¹‰æ˜ç¡®çš„è¿”å›ç±»å‹æ¨¡å¼ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹å¯ä»¥å‡†ç¡®è¯†åˆ«é¢„æœŸå±æ€§ï¼Œæ¶ˆé™¤æ¨¡å‹åœ¨æ²¡æœ‰æ¨¡å¼çš„æƒ…å†µä¸‹æ ¹æ®ä¸å®Œæ•´æˆ–æ¨¡æ£±ä¸¤å¯çš„ä¿¡æ¯åšå‡ºå‡è®¾æ—¶å¯èƒ½å‡ºç°çš„æ½œåœ¨ä¸å‡†ç¡®ä¹‹å¤„ã€‚å› æ­¤ï¼Œè¿™æé«˜äº†å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§ï¼Œä»è€Œè·å¾—æ›´å¯é å’Œç²¾ç¡®çš„ç»“æœã€‚